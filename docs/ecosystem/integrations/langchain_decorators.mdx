# LangChain è£…é¥°å™¨ âœ¨

LangChain è£…é¥°å™¨æ˜¯ LangChain ä¹‹ä¸Šçš„ä¸€å±‚ï¼Œä¸ºç¼–å†™è‡ªå®šä¹‰çš„ langchain prompt å’Œ chain æä¾›äº†ç®€åŒ–çš„è¯­æ³•ç³– ğŸ­

åé¦ˆã€é—®é¢˜å’Œè´¡çŒ®è¯·åœ¨æ­¤å¤„æå‡ºï¼š
[ju-bezdek/langchain-decorators](https://github.com/ju-bezdek/langchain-decorators)



ä¸»è¦åŸåˆ™å’Œä¼˜åŠ¿ï¼š

- ä»¥æ›´å…· `Python é£æ ¼` çš„æ–¹å¼ç¼–å†™ä»£ç 
- ç¼–å†™å¤šè¡Œæç¤ºï¼Œä¸ä¼šå¹²æ‰°ä»£ç æµç¨‹çš„ç¼©è¿›
- åˆ©ç”¨ IDE å†…ç½®çš„ **æç¤º**ã€**ç±»å‹æ£€æŸ¥** å’Œ **å¼¹å‡ºæ¡†**ï¼Œå¿«é€ŸæŸ¥çœ‹å‡½æ•°çš„æç¤ºã€å‚æ•°ç­‰ä¿¡æ¯
- å……åˆ†åˆ©ç”¨ ğŸ¦œğŸ”— LangChain ç”Ÿæ€ç³»ç»Ÿçš„å…¨éƒ¨åŠŸèƒ½
- æ·»åŠ å¯¹**å¯é€‰å‚æ•°**çš„æ”¯æŒ
- é€šè¿‡å°†å‚æ•°ç»‘å®šåˆ°ä¸€ä¸ªç±»ï¼Œè½»æ¾å…±äº«æç¤ºä¹‹é—´çš„å‚æ•°



ä¸‹é¢æ˜¯ä½¿ç”¨**LangChain è£…é¥°å™¨ âœ¨**ç¼–å†™çš„ç®€å•ä»£ç ç¤ºä¾‹

``` python

@llm_prompt
def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers")->str:
    """
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    """
    return

# run it naturaly
write_me_short_post(topic="starwars")
# or
write_me_short_post(topic="starwars", platform="redit")
```

# å¿«é€Ÿå¼€å§‹
## å®‰è£…
```bash
pip install langchain_decorators
```

## ç¤ºä¾‹

å¼€å§‹çš„å¥½æ–¹æ³•æ˜¯æŸ¥çœ‹è¿™é‡Œçš„ç¤ºä¾‹ï¼š
 - [jupyter notebook](https://github.com/ju-bezdek/langchain-decorators/blob/main/example_notebook.ipynb)
 - [colab notebook](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)

# å®šä¹‰å…¶ä»–å‚æ•°
è¿™é‡Œæˆ‘ä»¬åªæ˜¯ä½¿ç”¨ `llm_prompt` è£…é¥°å™¨å°†å‡½æ•°æ ‡è®°ä¸ºæç¤ºï¼Œä»è€Œæœ‰æ•ˆåœ°å°†å…¶è½¬æ¢ä¸º LLMChainã€‚è€Œä¸æ˜¯è¿è¡Œå®ƒ


æ ‡å‡†çš„ LLMchain æ¯”ä»…ä»…æœ‰ inputs_variables å’Œ prompt å¤šå¾—å¤š... è¿™ä¸ªå®ç°ç»†èŠ‚åœ¨è£…é¥°å™¨ä¸­éšè—äº†èµ·æ¥ã€‚
ä¸‹é¢æ˜¯å®ƒçš„å·¥ä½œåŸç†ï¼š

1. ä½¿ç”¨**å…¨å±€è®¾ç½®**ï¼š

``` python
# define global settings for all prompty (if not set - chatGPT is the current default)
from langchain_decorators import GlobalSettings

GlobalSettings.define_settings(
    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally
    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming
)
```

2. ä½¿ç”¨é¢„å®šä¹‰çš„**æç¤ºç±»å‹**

``` python
#You can change the default prompt types
from langchain_decorators import PromptTypes, PromptTypeSettings

PromptTypes.AGENT_REASONING.llm = ChatOpenAI()

# Or you can just define your own ones:
class MyCustomPromptTypes(PromptTypes):
    GPT4=PromptTypeSettings(llm=ChatOpenAI(model="gpt-4"))

@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) 
def write_a_complicated_code(app_idea:str)->str:
    ...

```

3. ç›´æ¥åœ¨è£…é¥°å™¨ä¸­å®šä¹‰è®¾ç½®

``` python
from langchain.llms import OpenAI

@llm_prompt(
    llm=OpenAI(temperature=0.7),
    stop_tokens=["\nObservation"],
    ...
    )
def creative_writer(book_title:str)->str:
    ...
```

## ä¼ é€’å†…å­˜å’Œ/æˆ–å›è°ƒï¼š

åªéœ€åœ¨å‡½æ•°ä¸­å£°æ˜å®ƒä»¬ï¼ˆæˆ–ä½¿ç”¨ kwargs ä¼ é€’ä»»ä½•å†…å®¹ï¼‰

```python

@llm_prompt()
async def write_me_short_post(topic:str, platform:str="twitter", memory:SimpleMemory = None):
    """
    {history_key}
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    """
    pass

await write_me_short_post(topic="old movies")

```

# ç®€åŒ–æµå¼å¤„ç†

å¦‚æœæˆ‘ä»¬æƒ³è¦åˆ©ç”¨æµå¼å¤„ç†ï¼š
 - æˆ‘ä»¬éœ€è¦å°†æç¤ºå®šä¹‰ä¸ºå¼‚æ­¥å‡½æ•°
 - åœ¨è£…é¥°å™¨ä¸­å¼€å¯æµå¼å¤„ç†ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥å®šä¹‰å…·æœ‰æµå¼å¤„ç†çš„ PromptType
 - ä½¿ç”¨ StreamingContext æ¥æ•è·æµ

è¿™æ ·ï¼Œæˆ‘ä»¬åªéœ€æ ‡è®°è¦è¿›è¡Œæµå¼å¤„ç†çš„æç¤ºï¼Œè€Œä¸éœ€è¦è°ƒæ•´ä½¿ç”¨å“ªä¸ª LLMï¼Œå°†æµå¤„ç†å¤„ç†ç¨‹åºä¼ é€’åˆ°é“¾çš„ç‰¹å®šéƒ¨åˆ†... åªéœ€åœ¨æç¤º/æç¤ºç±»å‹ä¸Šå¼€å¯/å…³é—­æµå¼å¤„ç†...

åªæœ‰åœ¨æµä¸Šä¸‹æ–‡ä¸­è°ƒç”¨æ—¶æ‰ä¼šå‘ç”Ÿæµå¼å¤„ç†... åœ¨é‚£é‡Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥å¤„ç†æµ

``` python
# this code example is complete and should run as it is

from langchain_decorators import StreamingContext, llm_prompt

# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)
# note that only async functions can be streamed (will get an error if it's not)
@llm_prompt(capture_stream=True) 
async def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):
    """
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    """
    pass



# just an arbitrary  function to demonstrate the streaming... wil be some websockets code in the real world
tokens=[]
def capture_stream_func(new_token:str):
    tokens.append(new_token)

# if we want to capture the stream, we need to wrap the execution into StreamingContext... 
# this will allow us to capture the stream even if the prompt call is hidden inside higher level method
# only the prompts marked with capture_stream will be captured here
with StreamingContext(stream_to_stdout=True, callback=capture_stream_func):
    result = await run_prompt()
    print("Stream finished ... we can distinguish tokens thanks to alternating colors")


print("\nWe've captured",len(tokens),"tokensğŸ‰\n")
print("Here is the result:")
print(result)
```


# æç¤ºå£°æ˜
é»˜è®¤æƒ…å†µä¸‹ï¼Œæç¤ºæ˜¯å‡½æ•°çš„æ•´ä¸ªæ–‡æ¡£ï¼Œé™¤éæ‚¨æ ‡è®°äº†æ‚¨çš„æç¤º

## è®°å½•æ‚¨çš„æç¤º

æˆ‘ä»¬å¯ä»¥æŒ‡å®šæˆ‘ä»¬æ–‡æ¡£çš„å“ªä¸ªéƒ¨åˆ†æ˜¯æç¤ºå®šä¹‰ï¼Œé€šè¿‡ä½¿ç”¨å¸¦æœ‰ `<prompt>` è¯­è¨€æ ‡ç­¾çš„ä»£ç å—

``` python
@llm_prompt
def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):
    """
    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.

    It needs to be a code block, marked as a `<prompt>` language
    ```<prompt>
    ä¸ºæˆ‘çš„å…³äº {topic} åœ¨ {platform} å¹³å°ä¸Šçš„å¸–å­æ’°å†™ä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ã€‚
    å®ƒåº”è¯¥é¢å‘ {audience} çš„è§‚ä¼—ã€‚
    ï¼ˆæœ€å¤š 15 ä¸ªè¯ï¼‰
    ```

    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.
    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))
    """
    return 
```

## èŠå¤©æ¶ˆæ¯æç¤º

å¯¹äºèŠå¤©æ¨¡å‹æ¥è¯´ï¼Œå°†æç¤ºå®šä¹‰ä¸ºä¸€ç»„æ¶ˆæ¯æ¨¡æ¿éå¸¸æœ‰ç”¨... è¿™æ˜¯å¦‚ä½•åšçš„ï¼š

``` python
@llm_prompt
def simulate_conversation(human_input:str, agent_role:str="a pirate"):
    """
    ## System message
     - note the `:system` sufix inside the <prompt:_role_> tag
     

    ```<prompt:system>
    ä½ æ˜¯ä¸€ä¸ª {agent_role} é»‘å®¢ã€‚ä½ å¿…é¡»åƒä¸€ä¸ªé»‘å®¢ä¸€æ ·è¡ŒåŠ¨ã€‚
    ä½ æ€»æ˜¯ç”¨ä»£ç å›å¤ï¼Œä½¿ç”¨ Python æˆ– JavaScript ä»£ç å—...
    ä¾‹å¦‚ï¼š
    
    ... ä¸è¦å›å¤å…¶ä»–ä»»ä½•ä¸œè¥¿... åªå›å¤ä»£ç  - ä¿æŒä½ çš„è§’è‰²ã€‚
    ```

    # human message 
    (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)
    ``` <prompt:user>
    ä½ å¥½ï¼Œä½ æ˜¯è°
    ```
    a reply:
    

    ``` <prompt:assistant>
    \``` python <<- escaping inner code block with \ that should be part of the prompt
    def hello():
        print("Argh... hello you pesky pirate")
    \```
    ```
    
    we can also add some history using placeholder
    ```<prompt:placeholder>
    {history}
    ```
    ```<prompt:user>
    {human_input}
    ```

    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.
    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))
    """
    pass

```

è¿™é‡Œçš„è§’è‰²æ˜¯æ¨¡å‹çš„æœ¬æœºè§’è‰²ï¼ˆèŠå¤© GPT çš„åŠ©æ‰‹ã€ç”¨æˆ·ã€ç³»ç»Ÿï¼‰



# å¯é€‰éƒ¨åˆ†
- æ‚¨å¯ä»¥å®šä¹‰æ‚¨çš„æç¤ºçš„æ•´ä¸ªéƒ¨åˆ†ï¼Œè¿™äº›éƒ¨åˆ†åº”è¯¥æ˜¯å¯é€‰çš„
- å¦‚æœéƒ¨åˆ†ä¸­æœ‰ä»»ä½•è¾“å…¥ç¼ºå¤±ï¼Œæ•´ä¸ªéƒ¨åˆ†å°†ä¸ä¼šå‘ˆç°

è¿™ç§æƒ…å†µçš„è¯­æ³•å¦‚ä¸‹ï¼š

``` python
@llm_prompt
def prompt_with_optional_partials():
    """
    this text will be rendered always, but

    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | "")   ?}

    you can also place it in between the words
    this too will be rendered{? , but
        this  block will be rendered only if {this_value} and {this_value}
        is not empty?} !
    """
```


# è¾“å‡ºè§£æå™¨

- llm_prompt è£…é¥°å™¨ä¼šæ ¹æ®è¾“å‡ºç±»å‹è‡ªåŠ¨æ£€æµ‹æœ€ä½³è¾“å‡ºè§£æå™¨ï¼ˆå¦‚æœæœªè®¾ç½®ï¼Œåˆ™è¿”å›åŸå§‹å­—ç¬¦ä¸²ï¼‰
- åˆ—è¡¨ã€å­—å…¸å’Œ pydantic è¾“å‡ºä¹Ÿè¢«åŸç”Ÿæ”¯æŒï¼ˆè‡ªåŠ¨å¤„ç†ï¼‰

``` python
# this code example is complete and should run as it is

from langchain_decorators import llm_prompt

@llm_prompt
def write_name_suggestions(company_business:str, count:int)->list:
    """ Write me {count} good name suggestions for company that {company_business}
    """
    pass

write_name_suggestions(company_business="sells cookies", count=5)
```

## æ›´å¤æ‚çš„ç»“æ„

å¯¹äºå­—å…¸/ pydanticï¼Œæ‚¨éœ€è¦æŒ‡å®šæ ¼å¼åŒ–æŒ‡ä»¤... 
è¿™å¯èƒ½å¾ˆç¹çï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ‚¨å¯ä»¥è®©è¾“å‡ºè§£æå™¨æ ¹æ®æ¨¡å‹ï¼ˆpydanticï¼‰ä¸ºæ‚¨ç”ŸæˆæŒ‡ä»¤

``` python
from langchain_decorators import llm_prompt
from pydantic import BaseModel, Field


class TheOutputStructureWeExpect(BaseModel):
    name:str = Field (description="The name of the company")
    headline:str = Field( description="The description of the company (for landing page)")
    employees:list[str] = Field(description="5-8 fake employee names with their positions")

@llm_prompt()
def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:
    """ Generate a fake company that {company_business}
    {FORMAT_INSTRUCTIONS}
    """
    return

company = fake_company_generator(company_business="sells cookies")

# print the result nicely formatted
print("Company name: ",company.name)
print("company headline: ",company.headline)
print("company employees: ",company.employees)

```


# å°†æç¤ºç»‘å®šåˆ°å¯¹è±¡

``` python
from pydantic import BaseModel
from langchain_decorators import llm_prompt

class AssistantPersonality(BaseModel):
    assistant_name:str
    assistant_role:str
    field:str

    @property
    def a_property(self):
        return "whatever"

    def hello_world(self, function_kwarg:str=None):
        """
        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method
        """

    
    @llm_prompt
    def introduce_your_self(self)->str:
        """
        ```Â <prompt:system>
        ä½ æ˜¯ä¸€ä¸ªåä¸º {assistant_name} çš„åŠ©æ‰‹ã€‚
        ä½ çš„è§’è‰²æ˜¯æ‰®æ¼” {assistant_role}
        ```
        ```<prompt:user>
        ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼ˆä¸è¶…è¿‡ 20 ä¸ªè¯ï¼‰
        ```
        """

    

personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")

print(personality.introduce_your_self(personality))
```


# æ›´å¤šç¤ºä¾‹ï¼š

- è¿™äº›ä»¥åŠæ›´å¤šç¤ºä¾‹ä¹Ÿå¯ä»¥åœ¨[æ­¤å¤„çš„ colab notebook](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)ä¸­æ‰¾åˆ°
- åŒ…æ‹¬ä½¿ç”¨çº¯ langchain è£…é¥°å™¨é‡æ–°å®ç°çš„ [ReAct Agent](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp)

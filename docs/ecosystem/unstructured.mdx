import CodeBlock from "@theme/CodeBlock";


# 非结构化数据

本页面介绍如何在LangChain中使用[非结构化数据](https://unstructured.io)。

## 什么是非结构化数据？

非结构化是一个[开源](https://github.com/Unstructured-IO/unstructured)Python包，用于从原始文档中提取文本以用于机器学习应用。目前支持分区Word文档（.doc或.docx格式)，幻灯片（.ppt或.pptx格式)， Pdf ， html文件，图像，电子邮件（.eml或.msg格式)，电子书， markdown，和纯文本文件。

`unstructured`是一个Python包，不能直接与TS / JS一起使用，但是Unstructured还维护一个[REST API](https://github.com/Unstructured-IO/unstructured-api)以支持使用其他编程语言编写的预处理流水线。托管的Unstructured API的端点为`https://api.unstructured.io/general/v0/general`，或者您可以使用[此处](https://github.com/Unstructured-IO/unstructured-api#dizzy-instructions-for-using-the-docker-image)找到的说明在本地运行服务。

目前（截至2023年4月26日)， Unstructured API不需要API密钥。将来，API将需要API密钥。 [Unstructured文档页面](https://unstructured-io.github.io/unstructured/)会包括有关如何获取API密钥的说明（一旦可用)。

## 快速开始

您可以使用以下代码在`langchain`中使用非结构化数据。
将文件名替换为要处理的文件。
如果您正在本地运行容器，则将url切换为`http://127.0.0.1:8000/general/v0/general`。
有关详细信息，请查看[API文档页面](https://api.unstructured.io/general/docs)。


import SingleExample from "!!raw-loader!@examples/document_loaders/unstructured.ts";


<CodeBlock language="typescript">{SingleExample}</CodeBlock>


## 目录

您还可以使用`UnstructuredDirectoryLoader`从目录中加载所有文件，该类继承自 [`DirectoryLoader`](../modules/indexes/document_loaders/examples/file_loaders/directory.md):

import DirectoryExample from "!!raw-loader!@examples/document_loaders/unstructured_directory.ts";


<CodeBlock language="typescript">{DirectoryExample}</CodeBlock>


目前，`UnstructuredLoader`支持以下文档类型:

- 纯文本文件（`.txt` / `.text`)
- PDF（`.pdf`)
- Word文档（`.doc` / `.docx`)
- PowerPoints（`.ppt` / `.pptx`)
- 图像文件（`.jpg` / `.jpeg`)
- 电子邮件（`.eml` / `.msg`)
- HTML（`.html`)
- Markdown文件（`.md`)

`UnstructuredLoader`的输出将是一个类似以下内容的`Document`对象数组:

```typescript

[

  Document {

    pageContent: `Decoder: The decoder is also composed of a stack of N = 6

  identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a

  third sub-layer, wh

  ich performs multi-head attention over the output of the encoder stack. Similar to the encoder, we

  employ residual connections around each of the sub-layers, followed by layer normalization. We also

  modify the self

  -attention sub-layer in the decoder stack to prevent positions from attending to subsequent

  positions. This masking, combined with fact that the output embeddings are offset by one position,

  ensures that the predic

  tions for position i can depend only on the known outputs at positions less than i.`,

    metadata: {

      page_number: 3,

      filename: '1706.03762.pdf',

      category: 'NarrativeText'

    }

  },

  Document {

    pageContent: '3.2 Attention',

    metadata: { page_number: 3, filename: '1706.03762.pdf', category: 'Title'

  }

]

```


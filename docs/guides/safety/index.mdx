# 审查

使用LLMs的一个重要问题是它们可能会生成有害或不道德的文本。这是该领域的一个活跃研究领域。在这里，我们介绍一些受到这项研究启发的内置链，旨在使LLMs的输出更安全。

- [审查链](/docs/guides/safety/moderation)$：明确检查任何输出文本是否有害并标记它。
- [宪法链](/docs/guides/safety/constitutional_chain)$：使用一组原则提示模型应如何行事。
- [逻辑谬误链](/docs/guides/safety/logical_fallacy_chain)$：检查模型输出是否存在逻辑谬误以纠正任何偏差。
- [Amazon Comprehend审查链](/docs/guides/safety/amazon_comprehend_chain)$：使用[Amazon Comprehend](https://aws.amazon.com/comprehend/)检测和处理PII和有毒性。

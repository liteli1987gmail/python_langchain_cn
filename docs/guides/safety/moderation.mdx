# 审查
本笔记本演示了如何使用审查链以及几种常见的方法。审查链对于检测可能是仇恨、暴力等的文字非常有用。这对于用户输入以及语言模型的输出都很有用。一些API提供者，如OpenAI，[明确禁止](https://beta.openai.com/docs/usage-policies/use-case-policy)您或您的最终用户生成某些类型的有害内容。为了遵守这一规定（并且基本上防止您的应用程序造成伤害），您通常希望将审查链附加到任何LLM链上，以确保LLM生成的任何输出都不会有害。

如果传入审查链的内容有害，没有一种最佳处理方式，这可能取决于您的应用程序。有时您可能希望在链中抛出错误（并由应用程序处理）。其他时候，您可能希望向用户返回一些解释文字，说明文本是有害的。甚至可能有其他处理方式！我们将在本教程中涵盖所有这些处理方式。

import Example from "@snippets/modules/chains/additional/moderation.mdx"

<Example/>

# Caching
![LangChain](https://pica.zhimg.com/50/v2-56e8bbb52aa271012541c1fe1ceb11a2_r.gif 'LangChain中文网')

LangChain提供了LLM的可选缓存层。这对两个原因很有用$：

如果您经常多次请求相同的完成，它可以通过减少您对LLM提供者的API调用次数来为您节省资金。
它可以通过减少您对LLM提供者的API调用次数来加快应用程序的速度。

import CachingLLM from "@snippets/modules/model_io/models/llms/how_to/llm_caching.mdx"

<CachingLLM/>

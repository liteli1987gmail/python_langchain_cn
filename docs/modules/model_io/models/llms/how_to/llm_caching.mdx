# 缓存 llm_caching

![LangChain](https://pica.zhimg.com/50/v2-56e8bbb52aa271012541c1fe1ceb11a2_r.gif 'LangChain中文网')

LangChain 为 LLM 提供了一个可选的缓存层。这个功能有两个好处：

如果您经常多次请求相同的补全，它可以通过减少您对 LLM 提供者的 API 调用次数来节省费用。
它可以通过减少您对 LLM 提供者的 API 调用次数来加速您的应用程序。

import CachingLLM from "@snippets/modules/model_io/models/llms/how_to/llm_caching.mdx"

<CachingLLM/>

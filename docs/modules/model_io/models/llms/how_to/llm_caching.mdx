# 缓存 llm_caching
LangChain为LLM提供了一个可选的缓存层。这个功能有两个好处：

如果您经常多次请求相同的补全，它可以通过减少您对LLM提供者的API调用次数来节省费用。
它可以通过减少您对LLM提供者的API调用次数来加速您的应用程序。

import CachingLLM from "@snippets/modules/model_io/models/llms/how_to/llm_caching.mdx"

<CachingLLM/>

# Caching
LangChain为聊天模型提供可选的缓存层。这有两个用途：

它可以通过减少对LLM提供者的API调用次数来节省费用，如果您经常多次请求相同的完成。
它可以通过减少对LLM提供者的API调用次数来加快应用程序的速度。

import CachingChat from "@snippets/modules/model_io/models/chat/how_to/chat_model_caching.mdx"

<CachingChat/>

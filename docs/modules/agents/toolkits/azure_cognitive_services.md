# Azure Cognitive Services Toolkit

è¿™ä¸ªå·¥å…·åŒ…ç”¨æ¥ä¸ŽAzure Cognitive Services APIäº¤äº’ï¼Œå®žçŽ°ä¸€äº›å¤šæ¨¡æ€èƒ½åŠ›ã€‚

ç›®å‰è¿™ä¸ªå·¥å…·åŒ…ä¸­åŒ…å«å››ä¸ªå·¥å…·ï¼š

- AzureCogsImageAnalysisToolï¼šç”¨äºŽä»Žå›¾åƒä¸­æå–æ ‡é¢˜ã€å¯¹è±¡ã€æ ‡ç­¾å’Œæ–‡æœ¬ã€‚ï¼ˆæ³¨æ„ï¼šç”±äºŽä¾èµ–äºŽ`azure-ai-vision`åŒ…ï¼Œæ­¤å·¥å…·ç›®å‰å°šä¸æ”¯æŒMac OSï¼Œè¯¥åŒ…åªæ”¯æŒWindowså’ŒLinuxã€‚ï¼‰

- AzureCogsFormRecognizerToolï¼šç”¨äºŽä»Žæ–‡æ¡£ä¸­æå–æ–‡æœ¬ã€è¡¨æ ¼å’Œé”®å€¼å¯¹ã€‚

- AzureCogsSpeech2TextToolï¼šç”¨äºŽå°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ã€‚

- AzureCogsText2SpeechToolï¼šç”¨äºŽå°†æ–‡æœ¬åˆæˆä¸ºè¯­éŸ³ã€‚

é¦–å…ˆï¼Œæ‚¨éœ€è¦è®¾ç½®ä¸€ä¸ªAzureè´¦æˆ·å¹¶åˆ›å»ºä¸€ä¸ªCognitive Servicesèµ„æºã€‚æ‚¨å¯ä»¥æŒ‰ç…§[è¿™é‡Œ](https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows)çš„è¯´æ˜Žæ¥åˆ›å»ºä¸€ä¸ªèµ„æºã€‚

ç„¶åŽï¼Œæ‚¨éœ€è¦èŽ·å–èµ„æºçš„ç»ˆç»“ç‚¹ã€å¯†é’¥å’ŒåŒºåŸŸï¼Œå¹¶å°†å®ƒä»¬è®¾ç½®ä¸ºçŽ¯å¢ƒå˜é‡ã€‚æ‚¨å¯ä»¥åœ¨èµ„æºçš„"Keys and Endpoint"é¡µé¢ä¸­æ‰¾åˆ°å®ƒä»¬ã€‚

```python
# !pip install --upgrade azure-ai-formrecognizer > /dev/null
# !pip install --upgrade azure-cognitiveservices-speech > /dev/null

# For Windows/Linux
# !pip install --upgrade azure-ai-vision > /dev/null
```

```python
import os

os.environ["OPENAI_API_KEY"] = "sk-"
os.environ["AZURE_COGS_KEY"] = ""
os.environ["AZURE_COGS_ENDPOINT"] = ""
os.environ["AZURE_COGS_REGION"] = ""
```

## åˆ›å»ºå·¥å…·åŒ…

```python
from langchain.agents.agent_toolkits import AzureCognitiveServicesToolkit

toolkit = AzureCognitiveServicesToolkit()
```

```python
[tool.name for tool in toolkit.get_tools()]
```




    ['Azure Cognitive Services Image Analysis',
     'Azure Cognitive Services Form Recognizer',
     'Azure Cognitive Services Speech2Text',
     'Azure Cognitive Services Text2Speech']



## åœ¨Agentä¸­ä½¿ç”¨


```python
from langchain import OpenAI
from langchain.agents import initialize_agent, AgentType
```


```python
llm = OpenAI(temperature=0)
agent = initialize_agent(
    tools=toolkit.get_tools(),
    llm=llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
)
```



```python
agent.run(
    "What can I make with these ingredients?"
    "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"
)
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3m
    Action:
    ```
    {
      "action": "Azure Cognitive Services Image Analysis",
      "action_input": "https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png"
    }
    ```
    
    [0m
    Observation: [36;1m[1;3mCaption: a group of eggs and flour in bowls
    Objects: Egg, Egg, Food
    Tags: dairy, ingredient, indoor, thickening agent, food, mixing bowl, powder, flour, egg, bowl[0m
    Thought:[32;1m[1;3m I can use the objects and tags to suggest recipes
    Action:
    ```
    {
      "action": "Final Answer",
      "action_input": "You can make pancakes, omelettes, or quiches with these ingredients!"
    }
    ```[0m
    
    [1m> Finished chain.[0m
    




    'You can make pancakes, omelettes, or quiches with these ingredients!'





```python
audio_file = agent.run("Tell me a joke and read it out for me.")
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3mAction:
    ```
    {
      "action": "Azure Cognitive Services Text2Speech",
      "action_input": "Why did the chicken cross the playground? To get to the other slide!"
    }
    ```
    
    [0m
    Observation: [31;1m[1;3m/tmp/tmpa3uu_j6b.wav[0m
    Thought:[32;1m[1;3m I have the audio file of the joke
    Action:
    ```
    {
      "action": "Final Answer",
      "action_input": "/tmp/tmpa3uu_j6b.wav"
    }
    ```[0m
    
    [1m> Finished chain.[0m
    




    '/tmp/tmpa3uu_j6b.wav'





```python
from IPython import display

audio = display.Audio(audio_file)
display.display(audio)
```


```python

```

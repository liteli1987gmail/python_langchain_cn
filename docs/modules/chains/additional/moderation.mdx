# 审查 Moderation
本文档演示了如何使用审查链以及几种常见的方法。审查链用于检测可能含有仇恨、暴力等内容的文本。这对于对用户输入进行处理以及对语言模型的输出进行处理都非常有用。一些 API 供应商（如 OpenAI）[明确禁止](https://beta.openai.com/docs/usage-policies/use-case-policy)您或您的最终用户生成某些类型的有害内容。为了遵守这一规定（并且通常还可以防止您的应用程序造成伤害），您可能经常希望在任何 LLMChains 后附加一个审查链，以确保 LLM 生成的任何输出都不会有害。

如果传递到审查链中的内容是有害的，则没有一种最佳处理方式，这可能取决于您的应用程序。有时，您可能希望在 Chain 中抛出错误（并由您的应用程序处理该错误）。其他时候，您可能希望向用户返回一些说明，说明文本是有害的。甚至可能还有其他处理方式！在本教程中，我们将涵盖所有这些处理方式。

import Example from "@snippets/modules/chains/additional/moderation.mdx"

<Example/>

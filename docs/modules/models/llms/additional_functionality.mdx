---
sidebar_label: 附加功能
---

import CodeBlock from "@theme/CodeBlock";

import Example from "!!raw-loader!@examples/models/llm/llm.ts";

import DebuggingExample from "!!raw-loader!@examples/models/llm/llm_debugging.ts";

import StreamingExample from "!!raw-loader!@examples/models/llm/llm_streaming.ts";

import TimeoutExample from "!!raw-loader!@examples/models/llm/llm_timeout.ts";

import CancellationExample from "!!raw-loader!@examples/models/llm/llm_cancellation.ts";


# LLMs: 附加功能

我们为LLM提供了许多附加功能。在下面的大多数示例中，我们将使用 `OpenAI` LLM。然而，所有这些功能都适用于所有LLMs。

## 附加方法

LangChain为与LLMs交互提供了许多附加方法:

<CodeBlock language="typescript">{Example}</CodeBlock>


## 流式响应

某些LLMs提供流式响应。这意味着您可以在整个响应被返回之前开始处理它。这很有用，如果您想要在生成响应时向用户显示响应，或者如果你要处理正在生成的响应。
LangChain目前提供`OpenAI` LLM的流式传输:

<CodeBlock language="typescript">{StreamingExample}</CodeBlock>


## 缓存

LangChain为LLMs提供可选的缓存层。有两个原因很有用:

1. 如果你经常请求相同的完成多次，它可以通过减少你对LLM提供商的调用次数来节省您的费用。
2. 它可以通过减少你对LLM提供商的API调用次数来加速你的应用程序。

### 内存中的缓存

默认缓存存储在内存中。这意味着如果重新启动应用程序，则会清除缓存。

你可以在实例化LLM时传递`cache: true`来启用它。例如:

```typescript
import { OpenAI } from "langchain/llms/openai";



const model = new OpenAI({ cache: true });

```


### 使用Momento进行缓存


LangChain还提供了基于Momento的缓存。[Momento](https://gomomento.com)是一种分布式的服务器端无服务器缓存，不需要任何设置或基础设施维护。要使用它，您需要安装`@gomomento/sdk`软件包:


```bash npm2yarn

npm install @gomomento/sdk

```

接下来，您需要注册并创建API密钥。完成后，像这样实例化LLM时传递一个`cache`选项:
Next you'll need to sign up and create an API key. Once you've done that, pass a `cache` option when you instantiate the LLM like this:



import MomentoCacheExample from "!!raw-loader!@examples/cache/momento.ts";



<CodeBlock language="typescript">{MomentoCacheExample}</CodeBlock>



###使用Redis进行缓存


LangChain还提供了基于Redis的缓存。如果您想要在多个进程或服务器之间共享缓存，这将非常有用。要使用它，您需要安装`redis`软件包:


```bash npm2yarn

npm install redis

```



然后，当您实例化LLM时，可以通过传递一个`cache`选项来使用它。例如:


```typescript

import { OpenAI } from "langchain/llms/openai";

import { RedisCache } from "langchain/cache/redis";

import { createClient } from "redis";



// See https://github.com/redis/node-redis for connection options

const client = createClient();

const cache = new RedisCache(client);



const model = new OpenAI({ cache });

```

##添加超时
## Adding a timeout



默认情况下，LangChain将无限期地等待模型提供程序的响应。如果要添加超时，您可以在调用模型时传递一个以毫秒为单位的`timeout`选项。例如，对于OpenAI:


<CodeBlock language="typescript">{TimeoutExample}</CodeBlock>



## 取消请求


您可以在调用模型时传递一个`signal`选项来取消请求。例如，对于OpenAI:


<CodeBlock language="typescript">{CancellationExample}</CodeBlock>



请注意，如果底层提供程序公开了取消请求的选项，那么此操作仅会取消即将发出的请求。LangChain 将尽可能取消底层请求，否则它将取消响应的处理。

## Dealing with Rate Limits



一些LLM提供商有速率限制。如果您超过了速率限制，将会出现错误。为了帮助您应对这个问题，LangChain在实例化LLM时提供了“maxConcurrency”选项。该选项允许您指定要发送到LLM提供商的最大并发请求数。如果您超过了这个数字，LangChain会自动将您的请求排队，以在先前的请求完成后发送。


例如，如果您设置`maxConcurrency: 5`，那么LangChain每次只会向LLM提供商发送5个请求。如果您发送了10个请求，前5个将会立即发送，而后5个将会排队等待。一旦前5个请求中的一个完成了，队列中的下一个请求将会发送。


要使用此功能，只需在实例化LLM时传递`maxConcurrency:<数字>`。例如:


```typescript

import { OpenAI } from "langchain/llms/openai";



const model = new OpenAI({ maxConcurrency: 5 });

```



## 处理API错误


如果模型提供商从其API返回错误，默认情况下，LangChain将在指数回退上最多重试6次。这样可以实现错误恢复，而无需任何额外的努力。如果您想更改此行为，可以在实例化模型时传递`maxRetries`选项。例如:


```typescript

import { OpenAI } from "langchain/llms/openai";



const model = new OpenAI({ maxRetries: 10 });

```



## 订阅事件


特别是在使用代理时，在 LLM 处理提示时可能会有大量的后台交互。对于代理，响应对象包含一个 `intermediateSteps` 对象，您可以打印该对象来查看它所采取的步骤概述。如果这不够用，并且您想要看到与 LLM 的每个交互，您可以将回调传递给 LLM，以进行自定义日志记录（或任何其他您想要执行的操作），当模型通过这些步骤时。

如需了解可用事件的更多信息，请参阅文档中的[回调]部分(/docs/production/callbacks/)。#Callbacks


<CodeBlock language="typescript">{DebuggingExample}</CodeBlock>


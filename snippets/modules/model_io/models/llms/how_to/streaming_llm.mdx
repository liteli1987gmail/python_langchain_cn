目前，我们支持对 `OpenAI`、`ChatOpenAI` 和 `ChatAnthropic` 实现的流式传输。要使用流式传输，请使用一个实现了 `on_llm_new_token` 的 [`CallbackHandler`](https://github.com/hwchase17/langchain/blob/master/langchain/callbacks/base.py)。在这个示例中，我们使用的是 `StreamingStdOutCallbackHandler`。

```python
from langchain.llms import OpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler


llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)
resp = llm("Write me a song about sparkling water.")
```

<CodeOutputBlock lang="python">

```
    Verse 1
    I'm sippin' on sparkling water,
    It's so refreshing and light,
    It's the perfect way to quench my thirst
    On a hot summer night.
    
    Chorus
    Sparkling water, sparkling water,
    It's the best way to stay hydrated,
    It's so crisp and so clean,
    It's the perfect way to stay refreshed.
    
    Verse 2
    I'm sippin' on sparkling water,
    It's so bubbly and bright,
    It's the perfect way to cool me down
    On a hot summer night.
    
    Chorus
    Sparkling water, sparkling water,
    It's the best way to stay hydrated,
    It's so crisp and so clean,
    It's the perfect way to stay refreshed.
    
    Verse 3
    I'm sippin' on sparkling water,
    It's so light and so clear,
    It's the perfect way to keep me cool
    On a hot summer night.
    
    Chorus
    Sparkling water, sparkling water,
    It's the best way to stay hydrated,
    It's so crisp and so clean,
    It's the perfect way to stay refreshed.
```

</CodeOutputBlock>

如果使用 `generate`，我们仍然可以访问最终的 `LLMResult`。但是，目前不支持对流式传输的 `token_usage`。


```python
llm.generate(["Tell me a joke."])
```

<CodeOutputBlock lang="python">

```
    Q: What did the fish say when it hit the wall?
    A: Dam!


    LLMResult(generations=[[Generation(text='\n\nQ: What did the fish say when it hit the wall?\nA: Dam!', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {}, 'model_name': 'text-davinci-003'})
```

</CodeOutputBlock>
